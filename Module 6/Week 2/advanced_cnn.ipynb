{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dùng Pytorch để xây dựng và huấn luyện mô hình deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sử dụng PIL để xử lý dữ liệu ảnh\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 59\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data/weather-dataset/dataset'\n",
    "img_paths = []\n",
    "labels = []\n",
    "classes = {\n",
    "    label_idx : class_name \\\n",
    "    for label_idx, class_name in enumerate(sorted(os.listdir(root_dir)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_idx, class_name in classes.items():\n",
    "    class_dir = os.path.join(root_dir, class_name)\n",
    "    for img_filename in os.listdir(class_dir):\n",
    "        img_path = os.path.join(class_dir, img_filename)\n",
    "        img_paths.append(img_path)\n",
    "        labels.append(label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "test_size = 0.125\n",
    "is_shuffle = True\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    img_paths, labels,\n",
    "    test_size=val_size,\n",
    "    random_state=seed,\n",
    "    shuffle=is_shuffle,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=val_size,\n",
    "    random_state=seed,\n",
    "    shuffle=is_shuffle,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__ (self, X, y, transform = None):\n",
    "        self.transform = transform\n",
    "        self.img_paths = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(img, img_size=(224, 224)):\n",
    "    img = img.resize(img_size)\n",
    "    img = np.array(img)[..., :3]\n",
    "    img = torch.tensor(img).permute(2, 0, 1).float()\n",
    "    normalized_img = img / 255.0\n",
    "\n",
    "    return normalized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WeatherDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = WeatherDataset(X_val, y_val, transform=transform)\n",
    "test_dataset = WeatherDataset(X_test, y_test, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 512\n",
    "test_batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=train_batch_size, \n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=test_batch_size, \n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=test_batch_size, \n",
    "    shuffle=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x.clone()\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x += self.downsample(shortcut)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, residual_block, n_block_lst, n_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = self.create_layer(residual_block, 64, 64, n_block_lst[0], 1)\n",
    "        self.conv3 = self.create_layer(residual_block, 64, 128, n_block_lst[1], 2)\n",
    "        self.conv4 = self.create_layer(residual_block, 128, 256, n_block_lst[2], 2)\n",
    "        self.conv5 = self.create_layer(residual_block, 256, 512, n_block_lst[3], 2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(512, n_classes)\n",
    "\n",
    "    def create_layer(self, residual_block, in_channels, out_channels, n_blocks, stride):\n",
    "        blocks = []\n",
    "        first_block = residual_block(in_channels, out_channels, stride)\n",
    "        blocks.append(first_block)\n",
    "\n",
    "        for idx in range(1, n_blocks):\n",
    "            block = residual_block(out_channels, out_channels, stride)\n",
    "            blocks.append(block)\n",
    "\n",
    "        block_seq = nn.Sequential(*blocks)\n",
    "\n",
    "        return block_seq\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(list(classes.keys()))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = ResNet(ResidualBlock, [2, 2, 2, 2], n_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    loss = sum(losses) / len(losses)\n",
    "    acc = correct / total\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_train_losses = []\n",
    "\n",
    "        model.train()\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_train_losses.append(loss.item())\n",
    "        \n",
    "        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, train loss: {train_loss}, val loss: {val_loss}')\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "epochs = 25\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, train loss: 1.9091659784317017, val loss: 2.3863386497941126, val acc: 0.18135469774217042\n",
      "Epoch 2/25, train loss: 1.3054147164026897, val loss: 2.3946838947229607, val acc: 0.11507647487254188\n",
      "Epoch 3/25, train loss: 1.0354009403122797, val loss: 2.356418469617533, val acc: 0.14202476329206118\n",
      "Epoch 4/25, train loss: 0.8189971711900499, val loss: 2.0903421529503756, val acc: 0.3029861616897305\n",
      "Epoch 5/25, train loss: 0.6651414102978177, val loss: 1.3792547273427942, val acc: 0.5600873998543335\n",
      "Epoch 6/25, train loss: 0.5412250823444791, val loss: 1.1966868493446083, val acc: 0.6001456664238893\n",
      "Epoch 7/25, train loss: 0.4136869079536862, val loss: 1.1527143536612046, val acc: 0.616897305171158\n",
      "Epoch 8/25, train loss: 0.3254849314689636, val loss: 1.033951195000216, val acc: 0.6613255644573925\n",
      "Epoch 9/25, train loss: 0.24365728927983177, val loss: 1.1323745501942413, val acc: 0.6190823015294975\n",
      "Epoch 10/25, train loss: 0.1679245481888453, val loss: 1.0274927874290667, val acc: 0.6584122359796067\n",
      "Epoch 11/25, train loss: 0.12782413595252567, val loss: 1.0195798238869322, val acc: 0.6627822286962856\n",
      "Epoch 12/25, train loss: 0.13050487637519836, val loss: 1.0386911050178285, val acc: 0.6562272396212673\n",
      "Epoch 13/25, train loss: 0.08383703562948439, val loss: 0.9840869402816129, val acc: 0.6831755280407866\n",
      "Epoch 14/25, train loss: 0.05900802214940389, val loss: 0.9562330082232176, val acc: 0.6715222141296431\n",
      "Epoch 15/25, train loss: 0.055683322664764195, val loss: 0.9547352252831293, val acc: 0.6707938820101966\n",
      "Epoch 16/25, train loss: 0.04326827989684211, val loss: 0.9842964692344499, val acc: 0.6671522214129643\n",
      "Epoch 17/25, train loss: 0.038685481581423015, val loss: 0.9645305955132772, val acc: 0.6802621995630007\n",
      "Epoch 18/25, train loss: 0.030978977473245725, val loss: 0.9576426780847616, val acc: 0.6846321922796795\n",
      "Epoch 19/25, train loss: 0.026492131046122976, val loss: 0.9532516679704882, val acc: 0.6868171886380189\n",
      "Epoch 20/25, train loss: 0.0261320730464326, val loss: 0.9539152166351329, val acc: 0.6860888565185724\n",
      "Epoch 21/25, train loss: 0.022318957580460444, val loss: 0.9564479229062103, val acc: 0.6868171886380189\n",
      "Epoch 22/25, train loss: 0.01986822485923767, val loss: 0.9584033754762522, val acc: 0.6860888565185724\n",
      "Epoch 23/25, train loss: 0.01989798102941778, val loss: 0.9674794662657172, val acc: 0.6809905316824472\n",
      "Epoch 24/25, train loss: 0.01842014884783162, val loss: 0.960913824307364, val acc: 0.6875455207574654\n",
      "Epoch 25/25, train loss: 0.01568818185478449, val loss: 0.9578917643791715, val acc: 0.6831755280407866\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = fit(model, train_loader, val_loader, criterion, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on val/test dataset\n",
      "Val accuracy:  0.6831755280407866\n",
      "Test accuracy:  0.692167577413479\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = evaluate(\n",
    "    model,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    device\n",
    ")\n",
    "\n",
    "test_loss, test_acc = evaluate(\n",
    "    model,\n",
    "    test_loader,\n",
    "    criterion,\n",
    "    device\n",
    ")\n",
    "\n",
    "print('Evaluation on val/test dataset')\n",
    "print('Val accuracy: ', val_acc)\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
